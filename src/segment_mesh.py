"""
Enhanced Mesh Export for OpenMAP-T1 Segmentations with Detail Preservation

This module provides advanced mesh export capabilities with several strategies for 
preserving fine anatomical details in the generated meshes:

DETAIL PRESERVATION STRATEGIES:
===============================

1. RESAMPLING TO HIGHER RESOLUTION:
   - Use resample_to_mm parameter to increase voxel density
   - Example: (0.5, 0.5, 0.5) doubles resolution in each dimension
   - Example: (0.3, 0.3, 0.3) for very high detail
   - Results in smoother curves and finer surface features

2. CONSERVATIVE SMOOTHING:
   - Reduced default smoothing iterations (15 → 5)
   - Lower relaxation factor (0.1 → 0.05)
   - Preserves sharp anatomical features

3. MESH SUBDIVISION:
   - use_subdivision=True adds surface detail after marching cubes
   - Subdivides each triangle into smaller triangles
   - Increases surface smoothness without losing sharp features

4. ADJUSTABLE ISO-SURFACE THRESHOLD:
   - iso_surface_value parameter (default 0.5)
   - Lower values (0.4-0.45) may capture more boundary detail
   - Higher values (0.55-0.6) for cleaner surfaces

5. CONSERVATIVE ISLAND CLEANUP:
   - Reduced min_island_size for keeping smaller anatomical structures
   - Careful balance between noise removal and detail preservation

6. NO DECIMATION BY DEFAULT:
   - Preserves all triangles generated by marching cubes
   - Only use decimation if file size is a concern

USAGE EXAMPLES:
===============

# High detail export (recommended for detailed analysis):
export_high_detail_meshes(
    segmentation_data="path/to/segmentation.nii",
    output_dir="meshes/high_detail/",
    target_spacing_mm=(0.3, 0.3, 0.3)  # Very high resolution
)

# Custom detail settings:
export_multi_segment_mesh(
    segmentation_data="path/to/segmentation.nii",
    output_dir="meshes/custom/",
    smooth_iterations=1,              # Minimal smoothing
    resample_to_mm=(0.4, 0.4, 0.4),  # Higher resolution
    use_subdivision=True,             # Add surface detail
    iso_surface_value=0.45,           # More aggressive surface
    min_island_size=10                # Keep small structures
)
"""

import vtk
import numpy as np
import nibabel as nib
import os
import pandas as pd
from typing import Tuple, Union, Optional, List, Dict
from scipy import ndimage
from skimage import measure
from scipy.ndimage import zoom


def _parse_level_from_filename(filename: str) -> Optional[str]:
    """
    Parse level identifier from segmentation filename.

    Parameters:
    -----------
    filename : str
        Filename or path containing level information (e.g., "MRI_AX_C_Reg_Type1_Level5.nii")

    Returns:
    --------
    Optional[str]
        Level identifier (e.g., "Level5") or None if not found
    """
    import re

    # Extract just the filename if a full path is provided
    basename = os.path.basename(filename)

    # Look for pattern like "Level1", "Level2", etc.
    match = re.search(r'Level(\d+)', basename)
    if match:
        level_num = match.group(1)
        return f"Level{level_num}"

    return None


def _load_openmap_lookup_table(lookup_table_path: Optional[str] = None, level: Optional[str] = None) -> Dict[int, str]:
    """
    Load OpenMAP-T1 lookup table to map intensity values to anatomical region names.

    Parameters:
    -----------
    lookup_table_path : str, optional
        Path to the lookup table file. If None, tries to find it automatically based on level.
    level : str, optional
        Level identifier (e.g., "Level5"). If provided, loads from level/Level*.txt file.

    Returns:
    --------
    Dict[int, str]
        Dictionary mapping intensity values to region labels
    """

    # If level is specified, try to load from level/*.txt files
    if level is not None:
        level_file_paths = [
            f"level/{level}.txt",
            f"../level/{level}.txt",
            f"../../level/{level}.txt"
        ]

        for path in level_file_paths:
            if os.path.exists(path):
                try:
                    lookup_dict = {}
                    with open(path, 'r', encoding='utf-8') as f:
                        for line in f:
                            line = line.strip()
                            if line and '\t' in line:
                                parts = line.split('\t', 1)
                                if len(parts) == 2:
                                    try:
                                        intensity = int(parts[0])
                                        anatomy_name = parts[1].strip()
                                        lookup_dict[intensity] = anatomy_name
                                    except ValueError:
                                        continue

                    print(
                        f"Loaded {len(lookup_dict)} anatomical region names from {level} lookup table.")
                    return lookup_dict

                except Exception as e:
                    print(
                        f"Warning: Failed to load {level} lookup table from {path}: {e}")
                    continue

        print(f"Warning: {level} lookup table not found. Using numeric names.")
        return {}

    # Fallback to CSV if no level specified (backward compatibility)
    if lookup_table_path is None:
        # Try to find the lookup table automatically
        possible_paths = [
            "level/OpenMAP-T1_multilevel_lookup_table_dictionary.csv",
            "../level/OpenMAP-T1_multilevel_lookup_table_dictionary.csv",
            "../../level/OpenMAP-T1_multilevel_lookup_table_dictionary.csv"
        ]

        for path in possible_paths:
            if os.path.exists(path):
                lookup_table_path = path
                break

    if lookup_table_path is None or not os.path.exists(lookup_table_path):
        print("Warning: OpenMAP-T1 lookup table not found. Using numeric names.")
        return {}

    try:
        # Load the lookup table
        df = pd.read_csv(lookup_table_path)

        # Create mapping from ROI# to Label
        lookup_dict = {}
        for _, row in df.iterrows():
            roi_num = int(row['ROI#'])
            label = str(row['Label']).strip()
            lookup_dict[roi_num] = label

        print(
            f"Loaded {len(lookup_dict)} anatomical region names from lookup table.")
        return lookup_dict

    except Exception as e:
        print(
            f"Warning: Failed to load lookup table: {e}. Using numeric names.")
        return {}


def _cleanup_small_islands(binary_data: np.ndarray, min_size: int = 100, keep_largest_only: bool = False) -> np.ndarray:
    """
    Remove small disconnected components (islands) from binary segmentation data.

    Parameters:
    -----------
    binary_data : np.ndarray
        3D binary array (0 and 1 values)
    min_size : int, default=100
        Minimum size (in voxels) for components to keep
    keep_largest_only : bool, default=False
        If True, keeps only the largest connected component

    Returns:
    --------
    np.ndarray
        Cleaned binary data with small islands removed
    """

    if np.sum(binary_data) == 0:
        return binary_data

    # Label connected components
    labeled_array, num_features = ndimage.label(binary_data)

    if num_features == 0:
        return binary_data

    # Calculate component sizes
    component_sizes = np.bincount(labeled_array.ravel())
    component_sizes[0] = 0  # Background is label 0

    # Create cleaned array
    cleaned_data = np.zeros_like(binary_data)

    if keep_largest_only:
        # Keep only the largest component
        if num_features > 0:
            largest_label = np.argmax(component_sizes[1:]) + 1
            cleaned_data[labeled_array == largest_label] = 1
            print(
                f"   Kept largest component: {component_sizes[largest_label]:,} voxels")
    else:
        # Keep components above minimum size
        kept_components = 0
        total_kept_voxels = 0

        for label in range(1, num_features + 1):
            size = component_sizes[label]
            if size >= min_size:
                cleaned_data[labeled_array == label] = 1
                kept_components += 1
                total_kept_voxels += size

        removed_components = num_features - kept_components
        print(
            f"   Found {num_features} components, kept {kept_components} (≥{min_size} voxels), removed {removed_components}")
        if kept_components > 0:
            print(f"   Total kept voxels: {total_kept_voxels:,}")

    return cleaned_data


def _resample_data(data: np.ndarray, affine: np.ndarray, target_spacing: Tuple[float, float, float]) -> Tuple[np.ndarray, np.ndarray]:
    """
    Resample data to target voxel spacing for higher resolution.

    Parameters:
    -----------
    data : np.ndarray
        3D input data
    affine : np.ndarray
        4x4 affine transformation matrix
    target_spacing : Tuple[float, float, float]
        Target voxel spacing in mm (x, y, z)

    Returns:
    --------
    Tuple[np.ndarray, np.ndarray]
        Resampled data and updated affine matrix
    """
    # Extract current spacing from affine matrix
    current_spacing = np.sqrt(np.sum(affine[:3, :3] ** 2, axis=0))

    # Calculate zoom factors
    zoom_factors = current_spacing / np.array(target_spacing)

    print(f"   Current spacing: {current_spacing}")
    print(f"   Target spacing: {target_spacing}")
    print(f"   Zoom factors: {zoom_factors}")
    print(f"   Original shape: {data.shape}")

    # Resample using nearest neighbor for segmentation data
    resampled_data = zoom(data, zoom_factors, order=0, prefilter=False)

    print(f"   Resampled shape: {resampled_data.shape}")
    print(f"   Resolution increase: {np.prod(zoom_factors):.2f}x voxels")

    # Update affine matrix
    new_affine = affine.copy()
    new_affine[:3, :3] = new_affine[:3, :3] / zoom_factors.reshape(3, 1)

    return resampled_data, new_affine


def export_segmentation_mesh(
    segmentation_data: Union[str, np.ndarray, nib.Nifti1Image],
    output_path: str,
    segment_value: Optional[int] = None,
    smooth_iterations: int = 5,  # Reduced default for better detail preservation
    decimate_reduction: float = 0.0,
    output_format: str = "vtk",
    use_anatomical_names: bool = True,
    lookup_table_path: Optional[str] = None,
    cleanup_islands: bool = True,
    min_island_size: int = 100,
    keep_largest_only: bool = False,
    resample_to_mm: Optional[Tuple[float, float, float]] = None,
    iso_surface_value: float = 0.5,
    use_subdivision: bool = False,
    subdivision_iterations: int = 2
) -> None:
    """
    Export segmentation mesh using VTK Marching Cubes algorithm with enhanced detail preservation.

    Parameters:
    -----------
    segmentation_data : str, np.ndarray, or nibabel.Nifti1Image
        Input segmentation data. Can be:
        - Path to NIfTI file (.nii or .nii.gz)
        - 3D numpy array
        - NiBabel Nifti1Image object
    output_path : str
        Output file path for the mesh (without extension)
    segment_value : int, optional
        Specific segment value to extract. If None, extracts all non-zero values
    smooth_iterations : int, default=5
        Number of smoothing iterations to apply. Reduced default for better detail preservation
    decimate_reduction : float, default=0.0
        Mesh decimation reduction factor (0.0-1.0). 0.0 means no decimation
    output_format : str, default="vtk"
        Output format: "vtk", "stl", "ply", or "obj"
    use_anatomical_names : bool, default=True
        Whether to use anatomical region names in filename
    lookup_table_path : str, optional
        Path to lookup table file. If level is auto-detected from filename, 
        uses level/Level*.txt files automatically
    cleanup_islands : bool, default=True
        Whether to remove small disconnected components (islands)
    min_island_size : int, default=100
        Minimum size (in voxels) for components to keep
    keep_largest_only : bool, default=False
        If True, keeps only the largest connected component
    resample_to_mm : Tuple[float, float, float], optional
        Target voxel spacing in mm (x, y, z). E.g., (0.5, 0.5, 0.5) for higher resolution
    iso_surface_value : float, default=0.5
        Iso-surface threshold value for marching cubes. Lower values may capture more detail
    use_subdivision : bool, default=False
        Whether to apply mesh subdivision to increase surface detail
    subdivision_iterations : int, default=2
        Number of subdivision iterations if use_subdivision=True

    Returns:
    --------
    None
    """

    # Load anatomical names if requested
    anatomical_lookup = {}
    if use_anatomical_names:
        # Parse level from filename if it's a string path
        level = None
        if isinstance(segmentation_data, str):
            level = _parse_level_from_filename(segmentation_data)
        anatomical_lookup = _load_openmap_lookup_table(
            lookup_table_path, level)

    # Load segmentation data
    if isinstance(segmentation_data, str):
        # Load from file path
        if not os.path.exists(segmentation_data):
            raise FileNotFoundError(
                f"Segmentation file not found: {segmentation_data}")
        nii_image = nib.load(segmentation_data)
        data = nii_image.get_fdata()
        affine = nii_image.affine
    elif isinstance(segmentation_data, nib.Nifti1Image):
        # Use nibabel image directly
        data = segmentation_data.get_fdata()
        affine = segmentation_data.affine
    elif isinstance(segmentation_data, np.ndarray):
        # Use numpy array directly
        data = segmentation_data
        affine = np.eye(4)  # Identity affine if not provided
    else:
        raise TypeError(
            "segmentation_data must be a file path, numpy array, or NiBabel image")

    # Convert to appropriate data type
    data = data.astype(np.float32)

    # Apply resampling if requested for higher resolution
    if resample_to_mm is not None:
        print(
            f"Resampling data to {resample_to_mm} mm spacing for higher resolution...")
        data, affine = _resample_data(data, affine, resample_to_mm)

    # Extract specific segment value if specified
    if segment_value is not None:
        binary_data = (data == segment_value).astype(np.float32)

        # Check if this segment exists in the data
        voxel_count = np.sum(binary_data > 0)
        if voxel_count == 0:
            print(
                f"Warning: No voxels found for segment {segment_value}. Skipping...")
            return

        print(f"Processing segment {segment_value}: {voxel_count} voxels")

        # Clean up small islands if requested
        if cleanup_islands:
            print("   Cleaning up small islands...")
            binary_data = _cleanup_small_islands(
                binary_data,
                min_size=min_island_size,
                keep_largest_only=keep_largest_only
            )
            # Update voxel count after cleanup
            voxel_count_after = np.sum(binary_data > 0)
            if voxel_count_after == 0:
                print(
                    f"   Warning: No voxels remaining after island cleanup. Skipping...")
                return
            elif voxel_count_after != voxel_count:
                print(
                    f"   Voxels after cleanup: {voxel_count_after} (removed {voxel_count - voxel_count_after})")

        # Create anatomical filename if lookup table is available
        if use_anatomical_names and segment_value in anatomical_lookup:
            anatomical_name = anatomical_lookup[segment_value]
            # Clean the anatomical name for filename use
            safe_name = _clean_filename(anatomical_name)
            output_path = f"{output_path}_{segment_value:03d}_{safe_name}"
        elif segment_value is not None:
            output_path = f"{output_path}_segment_{segment_value:03d}"
    else:
        binary_data = (data > 0).astype(np.float32)
        voxel_count = np.sum(binary_data > 0)
        if voxel_count == 0:
            print("Warning: No non-zero voxels found in data. Skipping...")
            return
        print(f"Processing all segments: {voxel_count} voxels")

        # Clean up small islands if requested
        if cleanup_islands:
            print("   Cleaning up small islands...")
            binary_data = _cleanup_small_islands(
                binary_data,
                min_size=min_island_size,
                keep_largest_only=keep_largest_only
            )
            # Update voxel count after cleanup
            voxel_count_after = np.sum(binary_data > 0)
            if voxel_count_after == 0:
                print(
                    "   Warning: No voxels remaining after island cleanup. Skipping...")
                return
            elif voxel_count_after != voxel_count:
                print(
                    f"   Voxels after cleanup: {voxel_count_after} (removed {voxel_count - voxel_count_after})")

    try:
        # Create VTK image data
        vtk_data = _numpy_to_vtk_image(binary_data, affine)

        # Apply Marching Cubes with configurable iso-surface value
        marching_cubes = vtk.vtkMarchingCubes()
        marching_cubes.SetInputData(vtk_data)
        # Configurable iso-surface value
        marching_cubes.SetValue(0, iso_surface_value)
        marching_cubes.Update()

        # Get the mesh
        mesh = marching_cubes.GetOutput()

        # Check if mesh has points
        num_points = mesh.GetNumberOfPoints()
        num_cells = mesh.GetNumberOfCells()

        if num_points == 0 or num_cells == 0:
            print(
                f"Warning: Marching cubes generated empty mesh (points: {num_points}, cells: {num_cells}). Skipping...")
            return

        print(f"Generated mesh: {num_points} points, {num_cells} triangles")

        # Apply smoothing if requested
        if smooth_iterations > 0:
            mesh = _smooth_mesh(mesh, smooth_iterations)
            print(f"Applied {smooth_iterations} smoothing iterations")

        # Apply subdivision if requested (for higher surface detail)
        if use_subdivision:
            original_cells = mesh.GetNumberOfCells()
            mesh = _subdivide_mesh(mesh, subdivision_iterations)
            new_cells = mesh.GetNumberOfCells()
            print(
                f"Subdivided mesh: {original_cells} → {new_cells} triangles ({subdivision_iterations} iterations)")

        # Apply decimation if requested
        if decimate_reduction > 0.0:
            original_cells = mesh.GetNumberOfCells()
            mesh = _decimate_mesh(mesh, decimate_reduction)
            new_cells = mesh.GetNumberOfCells()
            print(
                f"Decimated mesh: {original_cells} → {new_cells} triangles ({decimate_reduction*100:.1f}% reduction)")

        # Save the mesh
        _save_mesh(mesh, output_path, output_format)

        print(
            f"✓ Mesh exported successfully to: {output_path}.{output_format}")

    except Exception as e:
        print(f"✗ Error during mesh generation: {e}")
        import traceback
        traceback.print_exc()
        raise


def export_multi_segment_mesh(
    segmentation_data: Union[str, np.ndarray, nib.Nifti1Image],
    output_dir: str,
    segment_values: Optional[List[int]] = None,
    smooth_iterations: int = 5,  # Reduced default for better detail preservation
    decimate_reduction: float = 0.0,
    output_format: str = "vtk",
    use_anatomical_names: bool = True,
    lookup_table_path: Optional[str] = None,
    cleanup_islands: bool = True,
    min_island_size: int = 100,
    keep_largest_only: bool = False,
    resample_to_mm: Optional[Tuple[float, float, float]] = None,
    iso_surface_value: float = 0.5,
    use_subdivision: bool = False,
    subdivision_iterations: int = 2
) -> None:
    """
    Export multiple segmentation meshes for different segment values.

    Parameters:
    -----------
    segmentation_data : str, np.ndarray, or nibabel.Nifti1Image
        Input segmentation data
    output_dir : str
        Output directory for the meshes
    segment_values : List[int], optional
        List of segment values to extract. If None, extracts all unique non-zero values
    smooth_iterations : int, default=15
        Number of smoothing iterations to apply
    decimate_reduction : float, default=0.0
        Mesh decimation reduction factor (0.0-1.0)
    output_format : str, default="vtk"
        Output format: "vtk", "stl", "ply", or "obj"
    use_anatomical_names : bool, default=True
        Whether to use anatomical region names in filenames
    lookup_table_path : str, optional
        Path to lookup table file. If level is auto-detected from filename, 
        uses level/Level*.txt files automatically
    cleanup_islands : bool, default=True
        Whether to remove small disconnected components (islands)
    min_island_size : int, default=100
        Minimum size (in voxels) for components to keep
    keep_largest_only : bool, default=False
        If True, keeps only the largest connected component

    Returns:
    --------
    None
    """

    # Load anatomical names if requested
    anatomical_lookup = {}
    if use_anatomical_names:
        # Parse level from filename if it's a string path
        level = None
        if isinstance(segmentation_data, str):
            level = _parse_level_from_filename(segmentation_data)
        anatomical_lookup = _load_openmap_lookup_table(
            lookup_table_path, level)

    # Load segmentation data
    if isinstance(segmentation_data, str):
        print(f"Loading segmentation data from: {segmentation_data}")
        nii_image = nib.load(segmentation_data)
        data = nii_image.get_fdata()
        basename = os.path.splitext(os.path.basename(segmentation_data))[0]
        if basename.endswith('.nii'):
            basename = os.path.splitext(basename)[0]
    elif isinstance(segmentation_data, nib.Nifti1Image):
        data = segmentation_data.get_fdata()
        basename = "segmentation"
    elif isinstance(segmentation_data, np.ndarray):
        data = segmentation_data
        basename = "segmentation"
    else:
        raise TypeError(
            "segmentation_data must be a file path, numpy array, or NiBabel image")

    print(f"Data shape: {data.shape}, data type: {data.dtype}")
    print(f"Data range: min={np.min(data)}, max={np.max(data)}")
    print(f"Non-zero voxels: {np.sum(data > 0)}")

    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    print(f"Output directory: {output_dir}")

    # Get segment values to process
    if segment_values is None:
        segment_values = np.unique(data[data > 0]).astype(int).tolist()
        print(
            f"Auto-detected {len(segment_values)} unique segments: {segment_values[:10]}{'...' if len(segment_values) > 10 else ''}")
    else:
        print(
            f"Processing specified {len(segment_values)} segments: {segment_values}")

    successful_exports = 0
    failed_exports = 0

    # Process each segment
    for i, segment_value in enumerate(segment_values):
        print(
            f"\n--- Processing segment {i+1}/{len(segment_values)}: {segment_value} ---")

        # Check if segment exists in data
        segment_voxels = np.sum(data == segment_value)
        if segment_voxels == 0:
            print(f"⚠️  Segment {segment_value} has no voxels. Skipping.")
            continue

        # Create filename with anatomical name if available
        if use_anatomical_names and segment_value in anatomical_lookup:
            anatomical_name = anatomical_lookup[segment_value]
            safe_name = _clean_filename(anatomical_name)
            filename = f"{basename}_{segment_value:03d}_{safe_name}"
        else:
            filename = f"{basename}_segment_{segment_value:03d}"

        output_path = os.path.join(output_dir, filename)

        try:
            export_segmentation_mesh(
                segmentation_data=segmentation_data,
                output_path=output_path,
                segment_value=segment_value,
                smooth_iterations=smooth_iterations,
                decimate_reduction=decimate_reduction,
                output_format=output_format,
                use_anatomical_names=False,  # We already handled naming above
                lookup_table_path=lookup_table_path,
                cleanup_islands=cleanup_islands,
                min_island_size=min_island_size,
                keep_largest_only=keep_largest_only,
                resample_to_mm=resample_to_mm,
                iso_surface_value=iso_surface_value,
                use_subdivision=use_subdivision,
                subdivision_iterations=subdivision_iterations
            )

            if use_anatomical_names and segment_value in anatomical_lookup:
                print(
                    f"✓ Exported: {anatomical_lookup[segment_value]} (#{segment_value})")
            else:
                print(f"✓ Exported: Segment {segment_value}")

            successful_exports += 1

        except Exception as e:
            print(f"✗ Failed to process segment {segment_value}: {e}")
            failed_exports += 1
            continue

    print(f"\n=== Export Summary ===")
    print(f"Successful exports: {successful_exports}")
    print(f"Failed exports: {failed_exports}")
    print(
        f"Total processed: {successful_exports + failed_exports}/{len(segment_values)}")


def _clean_filename(name: str) -> str:
    """
    Clean anatomical region name for use in filename.

    Parameters:
    -----------
    name : str
        Original anatomical name

    Returns:
    --------
    str
        Cleaned filename-safe string
    """

    # Replace problematic characters
    cleaned = name.replace(" ", "_")
    cleaned = cleaned.replace("/", "_")
    cleaned = cleaned.replace("\\", "_")
    cleaned = cleaned.replace("(", "")
    cleaned = cleaned.replace(")", "")
    cleaned = cleaned.replace("-", "_")
    cleaned = cleaned.replace(".", "")
    cleaned = cleaned.replace(",", "")
    cleaned = cleaned.replace(":", "")
    cleaned = cleaned.replace(";", "")

    # Remove multiple underscores
    while "__" in cleaned:
        cleaned = cleaned.replace("__", "_")

    # Remove leading/trailing underscores
    cleaned = cleaned.strip("_")

    return cleaned


def _numpy_to_vtk_image(data: np.ndarray, affine: np.ndarray) -> vtk.vtkImageData:
    """
    Convert numpy array to VTK image data with proper spacing and origin.

    Parameters:
    -----------
    data : np.ndarray
        3D numpy array
    affine : np.ndarray
        4x4 affine transformation matrix

    Returns:
    --------
    vtk.vtkImageData
        VTK image data object
    """

    # Create VTK image data
    vtk_image = vtk.vtkImageData()

    # Set dimensions
    dims = data.shape
    vtk_image.SetDimensions(dims[0], dims[1], dims[2])

    # Extract spacing from affine matrix
    spacing = np.sqrt(np.sum(affine[:3, :3] ** 2, axis=0))
    vtk_image.SetSpacing(spacing[0], spacing[1], spacing[2])

    # Extract origin from affine matrix
    origin = affine[:3, 3]
    vtk_image.SetOrigin(origin[0], origin[1], origin[2])

    # Convert numpy array to VTK array - Fix the data transfer
    flat_data = data.ravel(order='F')  # Fortran order for VTK
    vtk_array = vtk.vtkFloatArray()
    vtk_array.SetNumberOfTuples(flat_data.size)
    vtk_array.SetName("scalars")

    # Copy data properly
    for i in range(flat_data.size):
        vtk_array.SetValue(i, float(flat_data[i]))

    # Set the array as scalars
    vtk_image.GetPointData().SetScalars(vtk_array)

    print(
        f"VTK image created: dimensions {dims}, {np.sum(data > 0)} non-zero voxels")

    return vtk_image


def _smooth_mesh(mesh: vtk.vtkPolyData, iterations: int) -> vtk.vtkPolyData:
    """
    Apply conservative Laplacian smoothing to the mesh while preserving details.

    Parameters:
    -----------
    mesh : vtk.vtkPolyData
        Input mesh
    iterations : int
        Number of smoothing iterations

    Returns:
    --------
    vtk.vtkPolyData
        Smoothed mesh
    """

    smoother = vtk.vtkSmoothPolyDataFilter()
    smoother.SetInputData(mesh)
    smoother.SetNumberOfIterations(iterations)
    # Reduced for better detail preservation
    smoother.SetRelaxationFactor(0.05)
    smoother.FeatureEdgeSmoothingOff()  # Preserve sharp features
    smoother.BoundarySmoothingOn()
    smoother.Update()

    return smoother.GetOutput()


def _subdivide_mesh(mesh: vtk.vtkPolyData, iterations: int) -> vtk.vtkPolyData:
    """
    Apply mesh subdivision to increase surface detail.

    Parameters:
    -----------
    mesh : vtk.vtkPolyData
        Input mesh
    iterations : int
        Number of subdivision iterations

    Returns:
    --------
    vtk.vtkPolyData
        Subdivided mesh with increased detail
    """

    subdivider = vtk.vtkLoopSubdivisionFilter()
    subdivider.SetInputData(mesh)
    subdivider.SetNumberOfSubdivisions(iterations)
    subdivider.Update()

    return subdivider.GetOutput()


def _decimate_mesh(mesh: vtk.vtkPolyData, reduction: float) -> vtk.vtkPolyData:
    """
    Apply mesh decimation to reduce polygon count.

    Parameters:
    -----------
    mesh : vtk.vtkPolyData
        Input mesh
    reduction : float
        Reduction factor (0.0-1.0)

    Returns:
    --------
    vtk.vtkPolyData
        Decimated mesh
    """

    decimator = vtk.vtkDecimatePro()
    decimator.SetInputData(mesh)
    decimator.SetTargetReduction(reduction)
    decimator.PreserveTopologyOn()
    decimator.Update()

    return decimator.GetOutput()


def _save_mesh(mesh: vtk.vtkPolyData, output_path: str, output_format: str) -> None:
    """
    Save mesh to file in specified format.

    Parameters:
    -----------
    mesh : vtk.vtkPolyData
        Mesh to save
    output_path : str
        Output file path (without extension)
    output_format : str
        Output format: "vtk", "stl", "ply", or "obj"
    """

    # Select appropriate writer
    if output_format.lower() == "vtk":
        writer = vtk.vtkPolyDataWriter()
        full_path = f"{output_path}.vtk"
    elif output_format.lower() == "stl":
        writer = vtk.vtkSTLWriter()
        full_path = f"{output_path}.stl"
    elif output_format.lower() == "ply":
        writer = vtk.vtkPLYWriter()
        full_path = f"{output_path}.ply"
    elif output_format.lower() == "obj":
        writer = vtk.vtkOBJWriter()
        full_path = f"{output_path}.obj"
    else:
        raise ValueError(f"Unsupported output format: {output_format}")

    # Configure and write
    writer.SetInputData(mesh)
    writer.SetFileName(full_path)
    writer.Write()


# Example usage function
def main():
    """
    Example usage of the mesh export functions.
    """

    # Example 1: Export single segment mesh with anatomical name
    # export_segmentation_mesh(
    #     segmentation_data="path/to/segmentation.nii.gz",
    #     output_path="output/brain_mesh",
    #     segment_value=75,  # Hippocampus Left
    #     smooth_iterations=15,
    #     decimate_reduction=0.1,
    #     output_format="stl",
    #     use_anatomical_names=True
    # )
    # Result: "brain_mesh_075_Hippo_L.stl"

    # Example 2: Export multiple segment meshes with anatomical names
    # export_multi_segment_mesh(
    #     segmentation_data="path/to/segmentation.nii.gz",
    #     output_dir="output/meshes/",
    #     segment_values=[75, 76, 83, 84],  # Hippocampi and Thalami
    #     smooth_iterations=10,
    #     decimate_reduction=0.05,
    #     output_format="stl",
    #     use_anatomical_names=True
    # )
    # Results: "segmentation_075_Hippo_L.stl", "segmentation_076_Hippo_R.stl", etc.

    pass


def export_high_detail_meshes(
    segmentation_data: Union[str, np.ndarray, nib.Nifti1Image],
    output_dir: str,
    segment_values: Optional[List[int]] = None,
    target_spacing_mm: Tuple[float, float, float] = (0.5, 0.5, 0.5),
    output_format: str = "stl"
) -> None:
    """
    Export high-detail meshes with optimized settings for maximum detail preservation.

    This is a convenience function that uses the best settings for detail preservation:
    - Resamples to higher resolution (0.5mm default)
    - Minimal smoothing (2 iterations)
    - No decimation
    - Mesh subdivision for extra surface detail
    - Conservative island cleanup

    Parameters:
    -----------
    segmentation_data : str, np.ndarray, or nibabel.Nifti1Image
        Input segmentation data
    output_dir : str
        Output directory for the meshes
    segment_values : List[int], optional
        List of segment values to extract. If None, extracts all unique non-zero values
    target_spacing_mm : Tuple[float, float, float], default=(0.5, 0.5, 0.5)
        Target voxel spacing in mm for high resolution
    output_format : str, default="stl"
        Output format: "vtk", "stl", "ply", or "obj"
    """

    print("🔬 Exporting HIGH DETAIL meshes with optimized settings:")
    print(f"   - Resampling to {target_spacing_mm} mm spacing")
    print("   - Minimal smoothing (2 iterations)")
    print("   - Mesh subdivision for extra surface detail")
    print("   - Conservative island cleanup")
    print("   - No decimation (preserves all triangles)")

    export_multi_segment_mesh(
        segmentation_data=segmentation_data,
        output_dir=output_dir,
        segment_values=segment_values,
        smooth_iterations=2,  # Minimal smoothing
        decimate_reduction=0.0,  # No decimation
        output_format=output_format,
        use_anatomical_names=True,
        cleanup_islands=True,
        min_island_size=25,  # More conservative cleanup
        keep_largest_only=False,
        resample_to_mm=target_spacing_mm,  # Higher resolution
        iso_surface_value=0.5,
        use_subdivision=True,  # Extra surface detail
        subdivision_iterations=1  # Conservative subdivision
    )


def export_openmap_meshes(openmap_output_dir: str, mesh_output_dir: str, file_basename: str):
    """
    Example function to export meshes from OpenMAP-T1 segmentation results.
    Now automatically uses anatomical region names!

    Parameters:
    -----------
    openmap_output_dir : str
        Directory containing OpenMAP-T1 output
    mesh_output_dir : str
        Directory to save mesh files
    file_basename : str
        Base name of the segmentation file (e.g., "MRI_AX_C_Reg")
    """

    # Path to Type1 Level5 segmentation (280 regions)
    segmentation_file = os.path.join(
        openmap_output_dir,
        file_basename,
        "parcellated",
        f"{file_basename}_Type1_Level5.nii"
    )

    if not os.path.exists(segmentation_file):
        print(f"Segmentation file not found: {segmentation_file}")
        return

    # Create mesh output directory
    mesh_dir = os.path.join(mesh_output_dir, f"{file_basename}_meshes")
    os.makedirs(mesh_dir, exist_ok=True)

    print(f"Processing OpenMAP-T1 segmentation: {segmentation_file}")

    # Example: Export meshes for key brain structures with automatic anatomical naming
    key_structures = [1, 2, 73, 74, 75, 76, 77, 78,
                      83, 84, 95, 96]  # Just intensity values

    print(
        f"Exporting {len(key_structures)} key brain structure meshes with anatomical names...")

    # Use the multi-segment export with anatomical naming
    export_multi_segment_mesh(
        segmentation_data=segmentation_file,
        output_dir=mesh_dir,
        segment_values=key_structures,
        smooth_iterations=10,
        decimate_reduction=0.1,
        output_format="stl",
        use_anatomical_names=True,  # This will automatically use anatomical names!
        cleanup_islands=True,  # Remove small disconnected components
        min_island_size=50,  # Keep components with ≥50 voxels
        keep_largest_only=False  # Keep all components above threshold
    )

    print(
        f"Mesh export completed with anatomical names! Files saved to: {mesh_dir}")


def test_mesh_export(segmentation_file: str = None):
    """
    Test function to debug mesh export issues.

    Parameters:
    -----------
    segmentation_file : str, optional
        Path to segmentation file. If None, tries to find OpenMAP-T1 output automatically.
    """

    print("=== Mesh Export Test ===")

    # Try to find a segmentation file if not provided
    if segmentation_file is None:
        possible_files = [
            "ouput/MRI_AX_C_Reg/parcellated/MRI_AX_C_Reg_Type1_Level5.nii",
            "output/MRI_AX_C_Reg/parcellated/MRI_AX_C_Reg_Type1_Level5.nii",
            "data/MRI_AX_C_Reg.nii.gz"
        ]

        for file_path in possible_files:
            if os.path.exists(file_path):
                segmentation_file = file_path
                break

    if segmentation_file is None or not os.path.exists(segmentation_file):
        print("❌ No segmentation file found for testing.")
        print("Please provide a path to a segmentation file.")
        return

    print(f"📁 Using segmentation file: {segmentation_file}")

    try:
        # Load and examine the data
        print("\n1. Loading data...")
        nii_image = nib.load(segmentation_file)
        data = nii_image.get_fdata()

        print(f"   Data shape: {data.shape}")
        print(f"   Data type: {data.dtype}")
        print(f"   Data range: {np.min(data)} to {np.max(data)}")
        print(f"   Non-zero voxels: {np.sum(data > 0):,}")

        # Find segments with reasonable number of voxels
        unique_values = np.unique(data[data > 0])
        print(f"   Unique segments: {len(unique_values)}")

        # Test with a specific segment that should have enough voxels
        test_segments = []
        for val in unique_values:
            voxel_count = np.sum(data == val)
            if voxel_count > 100:  # Only test segments with >100 voxels
                test_segments.append((int(val), voxel_count))
                if len(test_segments) >= 3:  # Test first 3 suitable segments
                    break

        if not test_segments:
            print("❌ No segments with sufficient voxels found (>100 voxels)")
            return

        print(f"\n2. Testing with segments: {[s[0] for s in test_segments]}")

        # Create test output directory
        test_dir = "test_meshes"
        os.makedirs(test_dir, exist_ok=True)

        # Test each segment
        for segment_value, voxel_count in test_segments:
            print(
                f"\n--- Testing segment {segment_value} ({voxel_count:,} voxels) ---")

            try:
                export_segmentation_mesh(
                    segmentation_data=segmentation_file,
                    output_path=os.path.join(
                        test_dir, f"test_segment_{segment_value}"),
                    segment_value=segment_value,
                    smooth_iterations=5,  # Reduced for faster testing
                    decimate_reduction=0.1,
                    output_format="stl",
                    use_anatomical_names=True,
                    cleanup_islands=True,  # Test island cleanup
                    min_island_size=20,  # Small threshold for testing
                    keep_largest_only=False
                )

                # Check if file was created
                expected_files = [
                    f"test_meshes/test_segment_{segment_value}_{segment_value:03d}_*.stl",
                    f"test_meshes/test_segment_{segment_value}_segment_{segment_value:03d}.stl"
                ]

                file_created = False
                for pattern in expected_files:
                    import glob
                    if glob.glob(pattern):
                        file_created = True
                        break

                if file_created:
                    print(
                        f"✅ SUCCESS: Mesh file created for segment {segment_value}")
                else:
                    print(
                        f"❌ FAILED: No mesh file found for segment {segment_value}")

            except Exception as e:
                print(f"❌ ERROR processing segment {segment_value}: {e}")
                import traceback
                traceback.print_exc()

        print(
            f"\n3. Test completed. Check '{test_dir}' directory for output files.")

    except Exception as e:
        print(f"❌ Test failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # Uncomment to run test
    test_mesh_export()

    # Uncomment to run examples

    # Example 1: Export meshes from OpenMAP-T1 output with anatomical names
    # export_openmap_meshes(
    #     openmap_output_dir="ouput/",  # OpenMAP-T1 output directory
    #     mesh_output_dir="meshes/",    # Where to save meshes
    #     file_basename="MRI_AX_C_Reg" # Base name from your data
    # )

    # Example 2: Export ALL segments as meshes with anatomical names
    # export_multi_segment_mesh(
    #     segmentation_data="ouput/MRI_AX_C_Reg/parcellated/MRI_AX_C_Reg_Type1_Level5.nii",
    #     output_dir="meshes/all_segments/",
    #     smooth_iterations=5,
    #     decimate_reduction=0.05,
    #     output_format="stl",
    #     use_anatomical_names=True  # Automatic anatomical naming!
    # )

    # Example 3: Export HIGH DETAIL meshes (NEW!)
    # export_high_detail_meshes(
    #     segmentation_data="ouput/MRI_AX_C_Reg/parcellated/MRI_AX_C_Reg_Type1_Level5.nii",
    #     output_dir="meshes/high_detail/",
    #     segment_values=[75, 76, 83, 84],  # Hippocampi and Thalami
    #     target_spacing_mm=(0.3, 0.3, 0.3),  # Very high resolution!
    #     output_format="stl"
    # )

    # Example 4: Custom high-detail settings
    # export_multi_segment_mesh(
    #     segmentation_data="ouput/MRI_AX_C_Reg/parcellated/MRI_AX_C_Reg_Type1_Level5.nii",
    #     output_dir="meshes/custom_detail/",
    #     segment_values=[1, 2, 73, 74],  # Select specific regions
    #     smooth_iterations=1,  # Minimal smoothing
    #     decimate_reduction=0.0,  # No decimation
    #     output_format="stl",
    #     use_anatomical_names=True,
    #     resample_to_mm=(0.4, 0.4, 0.4),  # Higher resolution
    #     iso_surface_value=0.45,  # Slightly more aggressive surface extraction
    #     use_subdivision=True,  # Add surface detail
    #     subdivision_iterations=2,  # More subdivision
    #     min_island_size=10  # Keep smaller structures
    # )

    main()
